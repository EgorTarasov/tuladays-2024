{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "from loguru import logger\n",
    "import loguru\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "import aiofiles\n",
    "import datetime as dt\n",
    "import asyncio\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.smclinic.ru\"\n",
    "DATA_PATH = pathlib.Path(\"./\") / \"data\"\n",
    "limits = httpx.Limits(max_keepalive_connections=400, max_connections=400)\n",
    "client = httpx.AsyncClient(timeout=300, limits=limits, follow_redirects=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH.absolute()\n",
    "logger.remove()\n",
    "\n",
    "# Add a new handler with the desired log level\n",
    "logger.add(sys.stderr, level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageStorage:\n",
    "    def __init__(self, base_path: pathlib.Path, website:str):\n",
    "        self.__website = website\n",
    "        self.__date_stamp = dt.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        self.__path = base_path / self.__date_stamp / self.__website\n",
    "        self.__path.mkdir(exist_ok=True, parents=True)\n",
    "        self.__html_path = self.__path / \"html\"\n",
    "        self.__html_path.mkdir(exist_ok=True, parents=True)\n",
    "        self.__txt_path = self.__path / \"txt\"\n",
    "        self.__txt_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    def get_html_path(self, url:str) -> pathlib.Path:\n",
    "        new_filename = \"_\".join(url.split(\"/\")[2:])\n",
    "        logger.debug(f\"filename on save {new_filename}\")\n",
    "        filename = self.__html_path / f\"{new_filename}.html\"\n",
    "        return filename\n",
    "\n",
    "    def check_page(self, url:str) -> bool:\n",
    "        filename = self.get_html_path(url)\n",
    "        return filename.exists()\n",
    "\n",
    "    async def save_page(self, url:str, content:str):\n",
    "        filename = self.get_html_path(url)\n",
    "        async with aiofiles.open(filename, \"w\") as f:\n",
    "            await f.write(content)\n",
    "        logger.info(f\"Page saved: {filename}\")\n",
    "\n",
    "    async def load_page(self, url:str) -> str:\n",
    "        filename = self.get_html_path(url)\n",
    "        async with aiofiles.open(filename, \"r\") as f:\n",
    "            content = await f.read()\n",
    "        return content\n",
    "\n",
    "    async def save_page_content(self, content: str , url : str):\n",
    "        filename = pathlib.Path(str(self.get_html_path(url)).replace(\".html\", \".txt\").replace(\"html\", \"txt\"))\n",
    "        async with aiofiles.open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            await f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = PageStorage(base_path=DATA_PATH, website=\"www.smclinic.ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_page(\n",
    "    client: httpx.AsyncClient, url: str, retries: int, delay: int\n",
    ") -> str:\n",
    "    response = await client.get(url)\n",
    "    if response.status_code >= 400:\n",
    "        if retries > 0:\n",
    "            logger.warning(f\"Retrying {url} ({retries} retries left)...\")\n",
    "            await asyncio.sleep(delay)\n",
    "            return await fetch_page(client, url, retries - 1, delay)\n",
    "        else:\n",
    "            logger.error(f\"unable to get page {response.status_code} {url}\")\n",
    "            return \"\"\n",
    "    return response.text\n",
    "\n",
    "\n",
    "async def get_page_content(\n",
    "    client: httpx.AsyncClient,\n",
    "    storage: PageStorage,\n",
    "    url: str,\n",
    "    retries: int = 3,\n",
    "    delay: int = 2,\n",
    ") -> str:\n",
    "    if storage.check_page(url):\n",
    "        logger.info(f\"Page {url} already exists\")\n",
    "        content = await storage.load_page(url)\n",
    "        return content\n",
    "\n",
    "    content = await fetch_page(client, url, retries, delay)\n",
    "    if content:\n",
    "        await storage.save_page(url, content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_diseases_list(client: httpx.AsyncClient, storage: PageStorage) -> list:\n",
    "    try:\n",
    "        page_data = await get_page_content(client, storage, BASE_URL + \"/diseases\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"unable to get page {e}\")\n",
    "        return []\n",
    "    soup = BeautifulSoup(page_data)\n",
    "\n",
    "    disease_div = soup.find(\"div\", {\"class\": \"diseases-list\"})\n",
    "    if not disease_div:\n",
    "        logger.error(\"unable to find disease list\")\n",
    "        return []\n",
    "    return disease_div.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Coroutine\n",
    "\n",
    "\n",
    "async def download_pages(client: httpx.AsyncClient, storage: PageStorage, pages: list[str]):\n",
    "    tasks: list[Coroutine[Any, Any, str]] = [\n",
    "        get_page_content(client, storage, url)\n",
    "        for url in pages\n",
    "    ]\n",
    "    results = await asyncio.gather(* tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disease_info(content: str):\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "    # b-text-block-6\n",
    "    content_divs = soup.find_all(\"div\", attrs={\"class\": \"b-text-block-6\"})\n",
    "    delimiter = \"\\n\" + \"-\" * 80 + \"\\n\"\n",
    "    result = delimiter.join(\n",
    "        [\n",
    "            block.text.strip()\n",
    "            for block in content_divs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = await get_diseases_list(client, storage)\n",
    "if not array is None:\n",
    "    print(len(array), array)\n",
    "pages = [\n",
    "    BASE_URL + elem.get(\"href\")\n",
    "    for elem in array\n",
    "    if elem.get(\"href\").startswith(\"/diseases\")\n",
    "]\n",
    "logger.info(f\"found: {len(pages)} pages in index page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await download_pages(client, storage, pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page_url in tqdm(pages):\n",
    "    try:\n",
    "        raw_content = await storage.load_page(page_url)\n",
    "    except FileNotFoundError as e:\n",
    "        raw_content = await get_page_content(client, storage, page_url)\n",
    "    page_content = get_disease_info(raw_content)\n",
    "    await storage.save_page_content(page_content, page_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_HTML_PATH = pathlib.Path(\"./\") / \"data\" / \"www.smclinic.ru\" / \"html\"\n",
    "NEW_TXT_PATH = pathlib.Path(\"./\") / \"data\" / \"www.smclinic.ru\" / \"txt\"\n",
    "\n",
    "CURRENT_PATH = pathlib.Path(\"./\") / \"data\" / \"www.smclinic.ru\"\n",
    "\n",
    "\n",
    "# move all html files to new folder\n",
    "for file in CURRENT_PATH.glob(\"*.html\"):\n",
    "    new_file = NEW_HTML_PATH / file.name\n",
    "    file.rename(new_file)\n",
    "    logger.info(f\"moved {file} to {new_file}\")\n",
    "\n",
    "# move all txt files to new folder\n",
    "for file in CURRENT_PATH.glob(\"*.txt\"):\n",
    "    new_file = NEW_TXT_PATH / file.name\n",
    "    file.rename(new_file)\n",
    "    logger.info(f\"moved {file} to {new_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
